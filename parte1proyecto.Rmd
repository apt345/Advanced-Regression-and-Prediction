---
title: "Statistical models for COVID-19 deaths forecasting and explanation"
author: "Arturo Prieto Tirado"
date: "29/4/2021"
output: html_document
---

<style>
body {
text-align: justify;
font-size: 9pt}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

## Introduction

The COVID-19 pandemic was first noticed in December 2019 and since then has propagated all around the world, leading all countries into trouble in both sanitary and economic ways. The evolution and propagation of the virus has been different all around the world since the countries themselves are different and so are the measures taken. The vaccination against the virus has already started but the pandemic has not finished yet and new variants of the virus might arise. Being able to predict when the peaks of the wave will happen accurately could be very useful knowledge against the virus. Furthermore, statistical models that could explain the drivers and quantify the effects of the different factors and countries' characteristics contributing to the expansion of the virus and deaths would be crucial to analyze whether the strategies taken by the different governments in the world have been useful and scientifically identify good choices to take in future waves of this virus or future pandemics that humanity might face. The present work aims to develop statistical models for explaining the main causes leading COVID-19 deaths and analyze its predictive accuracy, although an analysis based on Machine Learning models will later be done to focus on maximum predictive accuracy.

## Dataset description

The dataset was taken from:  

First thing is that in order to account for the seasonality of these events, one needs to work with lag variables. This is why the "smoothed" (7 day average) variables have been used instead of the daily ones. Furthermore, since this analysis includes different countries, it is important to normalize the data taking into account the total population of the country. Finally, the dataset contained many 

Since accounting for individual countries would not generalize well and generate noise, it has been chosen to work only with continents, with together with the month, can also serve as an indication of the season of the year. However, many countries have been erased to avoid micro-countries as well as full continents. The full list of countries selected for this project (even if country variable is not used) is:

Another variable, is island, was added to account for the fact that islands have it easier to close borders and protect themselves from the virus.

The selected variables thus are 

- New deaths smoothed per million: The target variable. Continuous variable representing the average of new deaths over 7 days per million inhabitants of the country.

- New cases smoothed per million: Continuous variable representing the average of new cases over 7 days per million inhabitants of the country.

- People fully vaccinated per hundred: Continuous variable representing the number of people vaccinated in that country per hundred inhabitants of the country, so the % of people fully vaccinated.

- Continent: Categorical variable representing the continent with six factors: Africa,
Asia, Europe, North America, Oceania and South America.

- Stringency Index: Continuous variable representing a measure of the level of restrictions a country has endured

- Population Density: Continuous variable representing the population density of the country

- Aged 65 or older: Continuous variable representing the percentage of the population aged 65 or older.

- Cardiovascular Death Rate: Continuous variable representing the cardiovascular death rate.

- Diabetes Prevalence: Continuous variable representing the diabetes prevalence.

- Life Expectancy: Continuous variable representing the life expectancy of the country.

- Human Development Index: Continuous variable representing the Human Development Index (HDI) of the country.

- Is island: Categorical variable of two levels accounting for the fact of whether the country is an island or not.

- Month: Categorical variable of 12 levels accounting for the month of the year 


Nas


And we can see their distribution in the following plots:


```{r}
library(caret)
library(doParallel)
library(tidyverse)
library(MASS)
library(VGAM)
library(e1071) 
library(gridExtra)
library(tictoc)
library(mice)
library(glmnet)
# Analysis


trainData = read.csv("C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/train_def.csv")
trainData=trainData[,-1]

testData = read.csv("C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/test_def.csv")
testData=testData[,-1]

#put categorical variables as factor
trainData$is_island=as.factor(trainData$is_island)
trainData$continent=as.factor(trainData$continent)
trainData$month=as.factor(trainData$month)

testData$is_island=as.factor(testData$is_island)
testData$continent=as.factor(testData$continent)
testData$month=as.factor(testData$month)

trainData %>% ggplot(aes_string(x = "new_deaths_smoothed_per_million")) +  geom_density(alpha = 0.2)
```

```{r}

plot_list=list()
plot_list2=list()
i=1
for(names in colnames(trainData)[c(2,4,5,6,7,8,9,10,11)]){
  plot_list[[i]]=trainData %>% ggplot(aes_string(x = names)) +  geom_density(alpha = 0.2)
  i=i+1
}

grid.arrange(grobs=plot_list,ncol=3)

```

```{r}
par(mfrow=c(1,3))
plot(trainData$continent)
plot(trainData$is_island)
plot(trainData$month)

```






## Statistical Models

A first step is to analyze the influence of the variables in deaths just by eye.
```{r}


# rewrite continent into dummy variables in order to make correlation easier
# it doesnt change anything else because the models work with dummies inside(?)
# crear otro dataset solo para correlaciones y dejar el otro como base(?)
# en el stepAIC importa porque al meterlo en una unica variable categorica no elimina
# uno a uno niveles no significativos, pero si los separamos sí
# leave Europe as base value

#same for months??? Another way to perform correlations??
#better to use ANOVA for cateogorical variables with many levels
# en todo caso esto son correlaciones y no tienen en cuenta efectos no lineales

data_correlations=trainData

data_correlations$continent_Asia="no"
data_correlations$continent_Asia[data_correlations$continent=="Asia"]="yes"
data_correlations$continent_Asia=as.factor(data_correlations$continent_Asia)

data_correlations$continent_Africa="no"
data_correlations$continent_Africa[data_correlations$continent=="Africa"]="yes"
data_correlations$continent_Africa=as.factor(data_correlations$continent_Africa)


data_correlations$continent_Oceania="no"
data_correlations$continent_Oceania[data_correlations$continent=="Oceania"]="yes"
data_correlations$continent_Oceania=as.factor(data_correlations$continent_Oceania)


data_correlations$continent_N_America="no"
data_correlations$continent_N_America[data_correlations$continent=="North America"]="yes"
data_correlations$continent_N_America=as.factor(data_correlations$continent_N_America)


data_correlations$continent_S_America="no"
data_correlations$continent_S_America[data_correlations$continent=="South America"]="yes"
data_correlations$continent_S_America=as.factor(data_correlations$continent_S_America)

meses=paste("Month",levels(data_correlations$month))
#all months minus one that is inherently contained in the others
for(i in 1:(length(meses)-1)){
  data_correlations[[meses[i]]]="no"
  data_correlations[[meses[i]]][data_correlations$month==i]="yes"
}
data_correlations=subset(data_correlations,select=-c(continent, month))


## First visualization and simple model/what to expect

leyenda = c("Continent", "New cases 7day p.m.", "New Deaths 7day p.m.",
            "Full vaccinations/100", "Stringency Index", "Population Density",
            "Aged 65 or older", "Cardiovascular death rate", "Diabetes prevalence",
            "Life expectancy", "HDI", "Is island?", "Month")

#plot each variable versus new deaths
par(mfrow=c(3,4))
for(i in 1:length(colnames(trainData))){
   if(i!=3){
      plot(x=trainData[,i], y=trainData$new_deaths_smoothed_per_million,
           xlab=leyenda[i], ylab="New deaths 7day p.m.")
   }
}
```

It is difficult to determine a simple mathematical relation between the variables just by looking at the plots since the data are very noisy. However, it can be seen that the number of deaths follows:

- Continent: Higher number of deaths in Europe and South America. Least in Oceania, which makes sense since they are islands.

- New cases: There appears to be some positive correlation, although there is a bit of noise.

- Full vaccinations: Although there is some noise when the vaccinations are zero (which is the first year of the pandemic, so it makes sense that there is a lot of change), clearly vaccinations reduce the number of deaths. This makes sense since most governments have first vaccinated old or ill-conditioned people who are the most susceptible to die.

- Stringency Index: There appears to be a positive correlations. This could be explained by thinking that the toughest measures were applied in the most difficult times.

- Population density: There appears to be a negative correlation. There are some countries with very high population density and few deaths (which might be islands as well). Intuitively, the more population density, the higher the transmission and the number of deaths. Although probably country total population density might not be the best real measure of population density and interpersonal contact of the cities.

- Aged 65 or older: It appears that the higher the percentage of old people, the higher the number of deaths, which seems intuitive. There are some data with the highest percentage of old people and few deaths, but this might be an outlier that follows another trend (an island for example).

- Cardiovascular death rate: Unexpectedly, the higher the cardiovascular risk, the lower the number of deaths. Maybe because people at risk took more measures that people with no pathologies that felt safe.

- Diabetes prevalence: Unexpectedly, the higher the diabetes prevalence, the lower the number of deaths. The same explanation as for cardiovascular death rate could be applied.

- Life expectancy: The higher the life expectancy, the higher the number of deaths. This is intuitive since the higher the life expectancy, the higher the number of old people.

- Human Development Index: The higher the HDI, the higher the number of deaths. High HDI corresponds to very developed countries, tipically with an old population.

- Is island?: Clearly the fact that islands can manage better their communications with the rest of the world and reduce transmission

- Month: Summer months present less deaths maybe because the virus gets milder but also since data are only for one summer and one should keep in mind that that was just after months of the hardest restrictions and lockdown.


Since finding transformations is a difficult task, one can 
```{r}
#plot(dates, y=trainData$new_deaths_smoothed_per_million)


# which are the most correlated variables with new deaths

library(ltm)
library(rcompanion)
y = data_correlations$new_deaths_smoothed_per_million
categorical_variables=c("continent_Asia", "continent_Oceania", "continent_Africa",
                        "continent_S_America", "continent_N_America", "is_island")
categorical_variables=append(categorical_variables, meses[1:11])

corr_label=matrix(nrow=length(data_correlations),ncol=length(data_correlations))
rownames(corr_label)=colnames(data_correlations)
colnames(corr_label)=colnames(data_correlations)
for(i in(1:(length(data_correlations)))){
   #print(colnames(data_correlations)[i])
   if(colnames(data_correlations)[i] %in% categorical_variables){
      corr_label["new_deaths_smoothed_per_million", colnames(data_correlations)[i]]=abs(biserial.cor(y, data_correlations[,i]))#take absolute value
   }else{
      corr_label["new_deaths_smoothed_per_million", colnames(data_correlations)[i]]=abs(cor(y, data_correlations[,i]))#take absolute value
   }
}
corr_label["new_deaths_smoothed_per_million", "new_deaths_smoothed_per_million"]=1#add manually the correlation with its own

corr_label <- sort(corr_label["new_deaths_smoothed_per_million",], decreasing = T)
corr=data.frame(corr_label)
ggplot(corr,aes(x = row.names(corr), y = corr_label)) + 
   geom_bar(stat = "identity", fill = "lightblue") + 
   scale_x_discrete(limits= row.names(corr)) +
   labs(x = "", y = "New Deaths 7day p.m.", title = "Correlations") + 
   theme(plot.title = element_text(hjust = 0, size = rel(1.5)),
         axis.text.x = element_text(angle = 45, hjust = 1))



```

We can see that the linear correlation is higher for new cases and aged 65 or older. In fact, a linear model with only those variables yields an R^2 of 0.55 while with all the variables, 0.60, which is not a very big improvement. However, the effectiveness of the linear model depends on the correct configuration of the variables. It is not simple to deduce a variable transformation since the plots confronting deaths versus the other variables are very noisy. This non-linearities might also be captured with Machine Learning models.

Let's first grasp the main idea with the most correlated variables doing a simple linear regression model that yields.

```{r}
simple_model=lm(new_deaths_smoothed_per_million~new_cases_smoothed_per_million+
                   aged_65_older+human_development_index, trainData)
#summary(simple_model)

#evaluate its performance
predictions <- predict(simple_model, newdata=testData)
# r squared on the test
#cor(testData$new_deaths_smoothed_per_million, predictions)^2
#RMSE
RMSE_simple_model <- sqrt(mean((predictions - testData$new_deaths_smoothed_per_million)^2))
#RMSE_simple_model
```

| $\beta_0$| $\beta_{Cases}$ | $\beta_{Age65}$ | $\beta_{HDI}$ | RMSE (test)| $R^2$ (test)|
|:-------------:|:------------:|:-----------:|:-----------:|:-------------:|:-----:|
|0.81 $\pm$ 0.04|(1.393 $\pm$ 0.006)$10^{-2}$|0.083 $\pm$ 0.002|-1.72 $\pm$ 0.07|1.75|0.56|

This means that increasing one unit the number of cases per million translates into 0.01393 more deaths. More cases mean more deaths, which seems intuitive. This coefficient also yields very useful information regarding COVID-19 mortality. For every COVID case, around 1.4% of people die. It can also be seen that a 1% increase in the percentage of population older than 65 leads to an increase of 0.083 average deaths per million, which seems intuitive but also a very small increase in deaths. Finally, the HDI coefficient shows that more developed countries tackle better the fight against COVID with less deaths, although HDI is a variable between 0 and 1, so the real differences between countries are small. With only these 3 variables the model is able to explain more than half of the variability, with an $R^2=0.56$.

But the real situation is more complex. In order to try to find the most relevant predictors, backwards regression with criteria such AIC can be used. Also, other methods like Lasso, Ridge Regression or Elastic Net will shrink the coefficients to 0 trying to find the most relevant ones. Dimension reduction techniques like Principal Component Regression or Partial Least squares will also be used. The following table shows the performance of the different techniques


```{r}
ctrl <- trainControl(method = "repeatedcv", 
                     number = 5, repeats = 4)

```

```{r, eval=False}
# full linear regression
full_model=lm(new_deaths_smoothed_per_million~., trainData)
summary(full_model)
#evaluate its performance
predictions <- predict(full_model, newdata=testData)
# r squared on the test
cor(testData$new_deaths_smoothed_per_million, predictions)^2
#RMSE
RMSE_simple_model <- sqrt(mean((predictions - testData$new_deaths_smoothed_per_million)^2))
RMSE_simple_model

```

```{r}
# backwards regression


```

```{r}
# Forward regression
for_tune <- train(new_deaths_smoothed_per_million~., data = trainData, 
                  method = "leapForward", 
                  preProc=c('scale', 'center'),
                  tuneGrid = expand.grid(nvmax = 1:25),
                  trControl = ctrl)
summary(for_tune)
for_tune


```


```{r}
# For penalized models use glmnet

# Lasso (alpha=0)
x = model.matrix(new_deaths_smoothed_per_million~., trainData)[,-1]
y = trainData$new_deaths_smoothed_per_million
#  Find the best lambda using cross-validation
set.seed(123) 
cv <- cv.glmnet(x, y, alpha = 0)
# Display the best lambda value
cv$lambda.min
# Fit the final model on the training data
model <- glmnet(x, y, alpha = 0, lambda = cv$lambda.min)
# Display regression coefficients
#coef(model)
# Make predictions on the test data
x.test <- model.matrix(new_deaths_smoothed_per_million ~., testData)[,-1]
predictions <- model %>% predict(x.test) %>% as.vector()
# Model performance metrics
data.frame(
  RMSE = RMSE(predictions, testData$new_deaths_smoothed_per_million),
  Rsquare = R2(predictions, testData$new_deaths_smoothed_per_million)
)
```


```{r}
# Ridge regression (alpha = 1 )
x = model.matrix(new_deaths_smoothed_per_million~., trainData)[,-1]
y = trainData$new_deaths_smoothed_per_million
#  Find the best lambda using cross-validation
set.seed(123) 
cv <- cv.glmnet(x, y, alpha = 1)
# Display the best lambda value
cv$lambda.min
# Fit the final model on the training data
model <- glmnet(x, y, alpha = 1, lambda = cv$lambda.min)
# Display regression coefficients
#coef(model)
# Make predictions on the test data
x.test <- model.matrix(new_deaths_smoothed_per_million ~., testData)[,-1]
predictions <- model %>% predict(x.test) %>% as.vector()
# Model performance metrics
data.frame(
  RMSE = RMSE(predictions, testData$new_deaths_smoothed_per_million),
  Rsquare = R2(predictions, testData$new_deaths_smoothed_per_million)
)
```

```{r}
# Elastic Net (alpha between 0 and 1)
x = model.matrix(new_deaths_smoothed_per_million~., trainData)[,-1]
y = trainData$new_deaths_smoothed_per_million
#  Find the best lambda using cross-validation
set.seed(123) 
cv <- cv.glmnet(x, y, alpha = 0.3)
# Display the best lambda value
cv$lambda.min
# Fit the final model on the training data
model <- glmnet(x, y, alpha = 0.3, lambda = cv$lambda.min)
# Display regression coefficients
#coef(model)
# Make predictions on the test data
x.test <- model.matrix(new_deaths_smoothed_per_million ~., testData)[,-1]
predictions <- model %>% predict(x.test) %>% as.vector()
# Model performance metrics
data.frame(
  RMSE = RMSE(predictions, testData$new_deaths_smoothed_per_million),
  Rsquare = R2(predictions, testData$new_deaths_smoothed_per_million)
)
```

```{r}
# Principal Component Regression

library(pls)

pcr.fit = pcr(full_model, data=trainData, ncomp = 8, validation = "LOO")
summary(pcr.fit)
plot(RMSEP(pcr.fit), legendpos = "topright")

pred = predict(pcr.fit, ncomp=4, newdata=testData)
y.test = log(testData$new_deaths_smoothed_per_million)
postResample(pred = pred,  obs = y.test)

```

```{r}
# Partial Least Squares

# With Caret:
pls_tune <- train(ModelF, data = training,
                  method='pls',
                  preProc=c('scale','center'),
                  tuneGrid = expand.grid(ncomp = 2:8),
                  trControl=ctrl)
plot(pls_tune)
test_results$pls <- predict(pls_tune, testing)
postResample(pred = test_results$pls,  obs = test_results$price)

```

| Model | RMSE (test) | $R^2$ (test)| Number of variables|
|:-----:|:----:|:--:|:------------------:|
|Simple L.R.|1.75 |0.56 |3|
|Full L. R.|1.68 |0.59 |12|
| Backwards regression| | |
| Lasso |1.685 |0.595 |
| Ridge |1.683 |0.595 |
| Elastic Net |1.683 |0.595 |
| PCR | | |
| PLS | | |


The best one is   which has the following coefficients:

| Model | $\beta_0$| $\beta_{Cases}$ |$\beta_{continent}$  | $\beta_{month}$| $\beta_{Age65}$ | $\beta_{HDI}$ | $\beta_{island}$| $\beta_{pop.dens}$ | $\beta_{l.exp}$ | $\beta_{Str.In.}$| $\beta_{Vac}$ | $\beta_{Diabetes}$ | $\beta_{Cardio}$ | RMSE |
|:-------------:|:-------------:|:------------:|:-----------:|:-----------:|:-------------:|:-------------:|:------------:|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|


This means that the explanation of the problem is:




Finally, we can use this knowledge not only to explain the main features that drive the pandemic's death but also to make predictions. The data used stop at march 2021, let's try to predict the situation now (May), obtaining a prediction interval for Spain's deaths. Take the best model and plug in Spain today's characteristics, which are:

```{r, eval=FALSE}
# So let's predict next month CO2 emissions:
new.data = data.frame(Year=2020, Month=as.factor(c(3)), CO2Lag1=data$CO2[length(data$CO2)])
predict(Model.season, newdata = new.data, interval = 'prediction')


```

One gets an estimated average deaths per million of  which in comparison with the average so far this month, , is a reasonable prediction.

## Discussion and Conclusions

Difficult to obtain exact variable transformations, maybe also because each country has its own optimal transformation.