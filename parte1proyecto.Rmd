---
title: "Statistical models for COVID-19 deaths forecasting and explanation"
author: "Arturo Prieto Tirado"
date: "29/4/2021"
output: html_document
---

<style>
body {
text-align: justify;
font-size: 10pt}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

## Introduction

The COVID-19 pandemic was first noticed in December 2019 and since then has propagated all around the world, leading all countries into trouble in both sanitary and economic ways. The evolution and propagation of the virus has been different all around the world since the countries themselves are different and so are the measures taken. The vaccination against the virus has already started but the pandemic has not finished yet and new variants of the virus might arise. Being able to predict when the peaks of the wave will happen accurately could be very useful knowledge against the virus. Furthermore, statistical models that could explain the drivers and quantify the effects of the different factors and countries' characteristics contributing to the expansion of the virus and deaths would be crucial to analyze whether the strategies taken by the different governments in the world have been useful and scientifically identify good choices to take in future waves of this virus or future pandemics that humanity might face. The present work aims to develop statistical models for explaining the main causes leading COVID-19 deaths and analyze its predictive accuracy, although an analysis based on Machine Learning models will later be done to focus on maximum predictive accuracy.

## Dataset description

The dataset was taken from the OurWordlInData repository: https://github.com/owid/covid-19-data/tree/master/public/data on the 14th of April 2021. Data were selected only from July 1st 2020 in advance since data from the first wave is not so reliable and the goal of this project is not to focus only on the first wave. In total, the data set contained 47263 observations.

First thing is that in order to account for the seasonality of these events, one needs to work with lag variables. This is why the "smoothed" (7 day average) variables have been used instead of the daily ones. Furthermore, since this analysis includes different countries, it is important to normalize the data taking into account the total population of the country, either by taking parts per hundred or per million inhabitants. Also, the dataset contained many variables that were very related with each other (like people above 65 and people above 70, clearly all the ones in the second group are inside the first) and only one of those was kept. Finally, features like admissions in ICU were discarded since they had an enormous amount of missing values (well above 50%).

On the other hand, since accounting for individual countries would not generalize well and generate noise, it has been chosen to work only with continents, that together with the month, can also serve as an indication of the season of the year (in case that has an impact on virus propagation). However, first, many countries have been erased to avoid micro-countries as well as full continents. The full list of entities present in the original dataset that have been discarded for this project (even if country variable is not used) is: Africa, Andorra, Anguilla, Antigua and Barbuda, Asia, Bermuda, Cayman Islands, Dominica, Europe, European Union, Faeroe Islands, Falkland Islands, Gibraltar, Grenada, Guernsey, International, Isle of Man, Jersey, Kosovo, Liechtenstein, Marshall Islands, Micronesia, Monaco, Montserrat, North America, Northern Cyprus, Oceania, Saint Helena, Saint Kitts and Nevis, Saint Lucia, Saint Vincent and the Grenadines, Samoa, San Marino, Sao Tome and Principe, Seychelles, Solomon Islands, Somalia, South America, South Sudan, Syria, Timor, Togo, Trinidad and Tobago, Turks and Caicos Islands, Vanuatu, Vatican, World.

Finally, another variable, is island, was added to account for the fact that islands have it easier to close borders and protect themselves from the virus.

The selected variables thus are 

- New deaths smoothed per million: The target variable. Continuous variable representing the average of new deaths over 7 days per million inhabitants of the country.

- New cases smoothed per million: Continuous variable representing the average of new cases over 7 days per million inhabitants of the country.

- People fully vaccinated per hundred: Continuous variable representing the number of people with all the require doses of a given vaccine in that country per hundred inhabitants of the country, so the % of people fully vaccinated.

- Continent: Categorical variable representing the continent with six factors: Africa,
Asia, Europe, North America, Oceania and South America.

- Stringency Index: Continuous variable representing a measure of the level of restrictions a country had endured at that time. It is composite measure based on 9 response indicators including school closures, workplace closures, and travel bans, rescaled to a value from 0 to 100 (100 = strictest response)

- Population Density: Continuous variable representing the population density of the country measured in people per square kilometers.

- Aged 65 or older: Continuous variable representing the percentage of the population aged 65 or older.

- Cardiovascular Death Rate: Continuous variable representing the death rate from cardiovascular disease in 2017 (annual number of deaths per 100,000 people).

- Diabetes Prevalence: Continuous variable representing the diabetes prevalence (% of population aged 20 to 79) in 2017.

- Life Expectancy: Continuous variable representing the life expectancy of the country at birth in 2019.

- Human Development Index: Continuous variable representing the Human Development Index (HDI) of the country which is a composite index measuring average achievement in three basic dimensions of human development—a long and healthy life, knowledge and a decent standard of living. Values for 2019.

- Is island: Categorical variable of two levels accounting for the fact of whether the country is an island or not.

- Month: Categorical variable of 10 levels accounting for the month of the year. Data were selected from July 1st 2020, meaning that May and June are not present.


It is important to note that some of these variables contained missing values. The way this has been fixed is by noting that NA in vaccines and cases appeared mostly when those were 0: very beginning of the pandemic in the world so many countries didn't have any and, in the case of vaccines, not until their development. This two cases were set to 0. Also, missing values on population density, HDI and other country features can be replaced just by a quick search on Google. Finally, the rest of the missing values were imputed using Random Forest imputer of the R package Mice that fits a model of a certain feature containing missing values in terms of the other features in the dataset in order to give an estimation of the missing value.

## Variable distribution

The distribution of the aforementioned variables can be seen in the following plots:


```{r}
library(caret)
library(doParallel)
library(tidyverse)
library(MASS)
library(VGAM)
library(e1071) 
library(gridExtra)
library(tictoc)
library(mice)
library(glmnet)
# Analysis


trainData = read.csv("C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/train_def_def.csv")
trainData=trainData[,-1]

testData = read.csv("C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/test_def_def.csv")
testData=testData[,-1]

#put categorical variables as factor
trainData$is_island=as.factor(trainData$is_island)
trainData$continent=as.factor(trainData$continent)
trainData$month=as.factor(trainData$month)

testData$is_island=as.factor(testData$is_island)
testData$continent=as.factor(testData$continent)
testData$month=as.factor(testData$month)
```

<p align="center">
```{r, fig.height=2.5, fig.width=2.5}
trainData %>% ggplot(aes_string(x = "new_deaths_smoothed_per_million")) +  geom_density(alpha = 0.2)
```
</p>

<p align="center">
```{r}

plot_list=list()
plot_list2=list()
i=1
for(names in colnames(trainData)[c(2,4,5,6,7,8,9,10,11)]){
  plot_list[[i]]=trainData %>% ggplot(aes_string(x = names)) +  geom_density(alpha = 0.2)
  i=i+1
}

grid.arrange(grobs=plot_list,ncol=3)

```
</p>

<p align="center">
```{r, fig.height=3}
par(mfrow=c(1,3))
plot(trainData$continent, main="Continent")
plot(trainData$is_island, main="Is island")
plot(trainData$month, main="Month")

```
</p>

In terms of the pandemic evolution, it can be seen that cases, deaths and vaccines have a very high density of zeroes, which makes sense since there are data from the beginning of the pandemic with zero values as well as many countries on which the pandemic arrived later in time, same as the vaccines, that didn't appear until the end of 2020. Also, regarding the stringency index, it can be seen that most of the countries have endured a moderate to high level of restrictions throughout all the pandemic.

In population density, there are clearly countries with very high population density (Macao, which is indeed like very big city and others). In terms of the population's characteristics, it can be seen that most of the countries have a low to moderate percentage of old people and the same for cardiovascular and diabetes pathologies. Life expectancy and HDI plots show that their modes are on high values, meaning that the dataset has more information on rich, developed countries that on poorer, less developed countries.

In terms of spatio temporal distribution, we can see that the most instances for a given continent are on African countries followed by Asian and European, this makes sense since these continents have a similar number of countries and if no special pattern on the data collection through time happened, they should follow the same trend, as happens with the variable is island (and Oceania), since there are not that many islands. Finally, the distribution through time is more or less uniform except for April. This is because data were downloaded from the web on the 14th of April and selected from July 2021, so April only has half of the month while the other have a full month period each.

## Statistical Models

A first step is to analyze the influence of the variables in deaths just by eye.
```{r}


# rewrite continent into dummy variables in order to make correlation easier
# it doesnt change anything else because the models work with dummies inside(?)
# crear otro dataset solo para correlaciones y dejar el otro como base(?)
# en el stepAIC importa porque al meterlo en una unica variable categorica no elimina
# uno a uno niveles no significativos, pero si los separamos sí
# leave Europe as base value

#same for months??? Another way to perform correlations??
#better to use ANOVA for cateogorical variables with many levels
# en todo caso esto son correlaciones y no tienen en cuenta efectos no lineales

data_correlations=trainData

data_correlations$continent_Asia="no"
data_correlations$continent_Asia[data_correlations$continent=="Asia"]="yes"
data_correlations$continent_Asia=as.factor(data_correlations$continent_Asia)

data_correlations$continent_Africa="no"
data_correlations$continent_Africa[data_correlations$continent=="Africa"]="yes"
data_correlations$continent_Africa=as.factor(data_correlations$continent_Africa)


data_correlations$continent_Oceania="no"
data_correlations$continent_Oceania[data_correlations$continent=="Oceania"]="yes"
data_correlations$continent_Oceania=as.factor(data_correlations$continent_Oceania)


data_correlations$continent_N_America="no"
data_correlations$continent_N_America[data_correlations$continent=="North America"]="yes"
data_correlations$continent_N_America=as.factor(data_correlations$continent_N_America)


data_correlations$continent_S_America="no"
data_correlations$continent_S_America[data_correlations$continent=="South America"]="yes"
data_correlations$continent_S_America=as.factor(data_correlations$continent_S_America)

meses=paste("Month",levels(data_correlations$month))
#all months minus one that is inherently contained in the others
for(i in 1:(length(meses)-1)){
  data_correlations[[meses[i]]]="no"
  data_correlations[[meses[i]]][data_correlations$month==i]="yes"
}
data_correlations=subset(data_correlations,select=-c(continent, month))


## First visualization and simple model/what to expect

leyenda = c("Continent", "New cases 7day p.m.", "New Deaths 7day p.m.",
            "Full vaccinations/100", "Stringency Index", "Population Density",
            "Aged 65 or older", "Cardiovascular death rate", "Diabetes prevalence",
            "Life expectancy", "HDI", "Is island?", "Month")
```

<p align="center">
```{r}
#plot each variable versus new deaths
par(mfrow=c(3,4))
for(i in 1:length(colnames(trainData))){
   if(i!=3){
      plot(x=trainData[,i], y=trainData$new_deaths_smoothed_per_million,
           xlab=leyenda[i], ylab="New deaths 7day p.m.")
   }
}
```
</p>

It is difficult to determine a simple mathematical relation between the variables just by looking at the plots since the data are very noisy. However, it can be seen that the number of deaths follows:

- Continent: Higher number of deaths in Europe and South America. Least in Oceania, which makes sense since they are islands.

- New cases: There appears to be some positive correlation, although there is a bit of noise.

- Full vaccinations: Although there is some noise when the vaccinations are zero (which is the first year of the pandemic, so it makes sense that there is a lot of change), clearly vaccinations reduce the number of deaths. This makes sense since most governments have first vaccinated old or ill-conditioned people who are the most susceptible to die.

- Stringency Index: There appears to be a positive correlations. This could be explained by thinking that the toughest measures were applied in the most difficult times.

- Population density: There appears to be a negative correlation. There are some countries with very high population density and few deaths (which might be islands as well). Intuitively, the more population density, the higher the transmission and the number of deaths. Although probably country total population density might not be the best real measure of population density and interpersonal contact of the cities. In fact, some countries like Macao (population density 20000 inhabitants/$\text{km}^2$) are similar to big cities in bigger countries and would reflect better the impact of interpersonal contact in contagion. It can be seen how Macao has indeed low deaths, which points to a necessity of change in the concept of population density. It will later be checked whether this variable is really significant.

- Aged 65 or older: It appears that the higher the percentage of old people, the higher the number of deaths, which seems intuitive. There are some data with the highest percentage of old people and few deaths, but this might be an outlier that follows another trend (an island for example).

- Cardiovascular death rate: Unexpectedly, the higher the cardiovascular risk, the lower the number of deaths. Maybe because people at risk took more measures that people with no pathologies that felt safe.

- Diabetes prevalence: Unexpectedly, the higher the diabetes prevalence, the lower the number of deaths. The same explanation as for cardiovascular death rate could be applied.

- Life expectancy: The higher the life expectancy, the higher the number of deaths. This is intuitive since the higher the life expectancy, the higher the number of old people.

- Human Development Index: The higher the HDI, the higher the number of deaths. High HDI corresponds to very developed countries, tipically with an old population.

- Is island?: Clearly the fact that islands can manage better their communications with the rest of the world and reduce transmission

- Month: Summer months present less deaths maybe because the virus gets milder but also since data are only for one summer and one should keep in mind that that was just after months of the hardest restrictions and lockdown.


Since finding transformations is a difficult task, one can look at the linear correlation (although losing possible non linear important terms). The following plot shows the correlation of the different predictors with the response variable (deaths) with categorical features taking as dummy variables (one hot encoding).


```{r}
#plot(dates, y=trainData$new_deaths_smoothed_per_million)


# which are the most correlated variables with new deaths

library(ltm)
library(rcompanion)
y = data_correlations$new_deaths_smoothed_per_million
categorical_variables=c("continent_Asia", "continent_Oceania", "continent_Africa",
                        "continent_S_America", "continent_N_America", "is_island")
categorical_variables=append(categorical_variables, meses[1:11])

corr_label=matrix(nrow=length(data_correlations),ncol=length(data_correlations))
rownames(corr_label)=colnames(data_correlations)
colnames(corr_label)=colnames(data_correlations)
for(i in(1:(length(data_correlations)))){
   #print(colnames(data_correlations)[i])
   if(colnames(data_correlations)[i] %in% categorical_variables){
      corr_label["new_deaths_smoothed_per_million", colnames(data_correlations)[i]]=abs(biserial.cor(y, data_correlations[,i]))#take absolute value
   }else{
      corr_label["new_deaths_smoothed_per_million", colnames(data_correlations)[i]]=abs(cor(y, data_correlations[,i]))#take absolute value
   }
}
corr_label["new_deaths_smoothed_per_million", "new_deaths_smoothed_per_million"]=1#add manually the correlation with its own

corr_label <- sort(corr_label["new_deaths_smoothed_per_million",], decreasing = T)
corr=data.frame(corr_label)
```


<p align="center">
```{r}
ggplot(corr,aes(x = row.names(corr), y = corr_label)) + 
   geom_bar(stat = "identity", fill = "lightblue") + 
   scale_x_discrete(limits= row.names(corr)) +
   labs(x = "", y = "New Deaths 7day p.m.", title = "Correlations") + 
   theme(plot.title = element_text(hjust = 0, size = rel(1.5)),
         axis.text.x = element_text(angle = 90, hjust = 1))

```
</p>

We can see that the linear correlation is higher for new cases and aged 65 or older as well as HDI, as could be seen also from the previous plots.

### Linear regression models

Let's first grasp the main idea with the most correlated variables doing a simple linear regression model that yields:

```{r}
test_results <- data.frame(new_deaths_smoothed_per_million=testData$new_deaths_smoothed_per_million)

```

```{r}
simple_model=lm(new_deaths_smoothed_per_million~new_cases_smoothed_per_million+ aged_65_older+human_development_index, trainData)
#summary(simple_model)

#save prediction
test_results$lm <- predict(simple_model, testData)
# evaluate its performance
#postResample(pred = test_results$lm,  obs = test_results$new_deaths_smoothed_per_million)

```

| $\beta_0$| $\beta_{Cases}$ | $\beta_{Age65}$ | $\beta_{HDI}$ | RMSE (test)| $R^2$ (test)|
|:-------------:|:------------:|:-----------:|:-----------:|:-------------:|:-----:|
|1.06 $\pm$ 0.05|(1.385 $\pm$ 0.006)$10^{-2}$|0.085 $\pm$ 0.002|-2.07 $\pm$ 0.09|1.94|0.62|

This means that increasing one unit the number of cases per million translates into 0.01385 more deaths per million. More cases mean more deaths, which seems intuitive. This coefficient also yields very useful information regarding COVID-19 mortality. For every COVID case, around 1.4% of people die. It can also be seen that a 1% increase in the percentage of population older than 65 leads to an increase of 0.085 deaths per million, which seems intuitive since old population may be more vulnerable. Finally, the HDI coefficient shows that more developed countries tackle better the fight against COVID with less deaths, although HDI is a variable between 0 and 1, so the real differences between countries are small. With only these 3 variables the model is able to explain more than half of the variability of the test set, with an $R^2=0.62$.

We can try to see the difference by fitting a linear model with all the variables. This full model has an $R^2=0.65$, which is not a very big improvement considering we went from 3 variables to 26 (including dummies). This means that if we want to improve the model further, we need to include some kind of non-linear term. Since identifying a pure transformation for the variables seems difficult, we would try first with one to one interactions between variables and leave the identification of non-linear trends to Machine Learning models.

A model with interactions was done but excluding interaction terms of month and vaccinations since when merging them with the other variables there were levels with zero variance. By doing this model with interactions one gets an $R^2=0.77$ which is a considerable improvement but at the cost of increasing the variable size up to 210 variables (including dummies). In order to try to find the most relevant predictors, several techniques will be used:

- Stepwise regression: with Backwards, Forward and Stepwise methods that try to find the best subset of variables.

- Penalization Algorithms: Like Lasso, Ridge Regression or Elastic Net which will shrink the coefficients related with the "useless" variables to 0 trying to find the most relevant ones.

- Dimension reduction techniques: like Principal Component Regression or Partial Least squares that attempt to simplify the predictors by using instead principal components (PCR) or giving more weight to the most correlated variables (interactions count as well as new predictors) with the response (PLS).


```{r}
ctrl <- trainControl(method = "repeatedcv", 
                     number = 5, repeats = 4)

```

```{r}
#remove response and vaccinations
varnames=colnames(trainData[,-c(3)])

# Formula allowing only interaction between two variables 
double_formula = c()
for (i in 1:(length(varnames))){
  for (j in 1:(length(varnames))){
    if(i<j & !(j %in% c(3,11))& !(i %in% c(3,11))){
       #print(j)
      double_formula = c(double_formula, paste(c(varnames[i], varnames[j]), collapse = ":"))  
    }
  }
}
double_interaction_formula = paste(c( varnames, paste(double_formula, collapse = "+")), collapse = "+")

final_formula=paste(c("new_deaths_smoothed_per_million~"), double_interaction_formula)
# full linear regression robust rlm
full_model=lm(as.formula(final_formula), trainData)
#summary(full_model)
#save prediction
test_results$full_lm <- predict(full_model, testData)
# evaluate its performance
#postResample(pred = test_results$full_lm,  obs = test_results$new_deaths_smoothed_per_million)

```

### Stepwise Regression

Let's first look at the stepwise algorithms. The following plots show the decrease of the Root Mean Squared Error (the lower, the better) with the introduction of more variables, estimated via cross validation. It can be seen that there is a point where the improvement becomes very slow and thus the remaining variables, not so relevant. Both tend to the same performance when the number of features is high (30 in advance) but the interesting idea is to keep the least variables as possible. Therefore, a reasonable number for explainability would be to keep around 15-16 variables of the stepwise model in particular.

```{r}
# backwards regression
# back_tune <- train(as.formula(final_formula), data = trainData,
#                    method = "leapBackward",
#                    preProc=c('scale', 'center'),
#                    tuneGrid = expand.grid(nvmax = 2:30),
#                    trControl = ctrl)
# save(back_tune, file="C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/back_tune.RData")
load("C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/back_tune.RData")
#back_tune$bestTune$nvmax
#back_tune
# which variables are selected?
#coef(back_tune$finalModel, back_tune$bestTune$nvmax)
test_results$bw <- predict(back_tune, testData)
#evaluate its performance
#postResample(pred = test_results$bw,  obs = test_results$new_deaths_smoothed_per_million)

#could work with 15 predictors
```

```{r}
# Forward regression
# for_tune <- train(as.formula(final_formula), data = trainData,
#                    method = "leapForward",
#                    preProc=c('scale', 'center'),
#                    tuneGrid = expand.grid(nvmax = 2:30),
#                    trControl = ctrl)
# save(for_tune, file="C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/for_tune.RData")
load("C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/for_tune.RData")
#for_tune
#for_tune$bestTune$nvmax
# which variables are selected?
#coef(for_tune$finalModel, for_tune$bestTune$nvmax)
test_results$forward <- predict(for_tune, testData)
#postResample(pred = test_results$forward,  obs = test_results$new_deaths_smoothed_per_million)

```

```{r}
# Stepwise regression
# step_tune <- train(as.formula(final_formula), data = trainData,
#                    method = "leapSeq",
#                    preProc=c('scale', 'center'),
#                    tuneGrid = expand.grid(nvmax = 1:20),
#                    trControl = ctrl)
# save(step_tune, file="C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/step_tune.RData")
load("C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/step_tune.RData")

# which variables are selected?
#coef(step_tune$finalModel, unlist(step_tune$bestTune))


test_results$seq <- predict(step_tune, testData)
#postResample(pred = test_results$seq,  obs = test_results$new_deaths_smoothed_per_million)


```

<p align="center">
```{r, fig.height=3, fig.width=3}
par(mfrow=c(1,3))
plot(back_tune, main= "Backwards")
plot(for_tune, main= "Forward")
plot(step_tune, main="Stepwise")

```
</p>



### Penalized Regression

Penalized regression tries to find the optimal value of the penalization parameters (one for ridge and lasso, two for elastic net) that penalize the size of the coefficients, thus shrinking to zero the unwanted ones. The following plot shows an example of how this works with the lasso, when it can be seen that the higher the penalization parameter, the more coefficients go to 0. The idea is to find the best combination between explainability and "effective" number of variables.

```{r}
# For penalized models use glmnet since it finds the optimal penalization parameter better than caret (in caret you need to specify the grid)

# Lasso (alpha=0)
x = model.matrix(as.formula(final_formula), trainData)[,-1]
y = trainData$new_deaths_smoothed_per_million
# #  Find the best lambda using cross-validation
 set.seed(123)
cv <- cv.glmnet(x, y, alpha = 0)
# # Display the best lambda value
# #cv$lambda.min
# # Fit the final model on the training data
# lasso <- glmnet(x, y, alpha = 0, lambda = cv$lambda.min)
# save(lasso, file="C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/lasso.RData")
load("C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/lasso.RData")
# Display regression coefficients divided by tge original one to see the shrinkage.
#coef(model)[coef(model)/coef(full_model)>0.8]
# Make predictions on the test data
x.test <- model.matrix(as.formula(final_formula), testData)[,-1]
predictions <- lasso %>% predict(x.test) %>% as.vector()
test_results$lasso=predictions
```

<p align="center">
```{r, fig.height=3}
plot(cv$glmnet.fit, xvar="lambda")
# Model performance metrics
# data.frame(
#   RMSE = RMSE(predictions, testData$new_deaths_smoothed_per_million),
#   Rsquare = R2(predictions, testData$new_deaths_smoothed_per_million)
# )
```
</p>

```{r}
# Ridge regression (alpha = 1 )
# x = model.matrix(as.formula(final_formula), trainData)[,-1]
# y = trainData$new_deaths_smoothed_per_million
# #  Find the best lambda using cross-validation
# set.seed(123)
# cv <- cv.glmnet(x, y, type.measure="mse", alpha = 1)
# # Display the best lambda value
# #cv$lambda.min
# # Fit the final model on the training data
# ridge <- glmnet(x, y, alpha = 1, lambda = cv$lambda.min)
# save(ridge, file="C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/ridge.RData")
load("C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/ridge.RData")

# Display regression coefficients
#coef(model)[coef(model)/coef(full_model)>0.8]
# Make predictions on the test data
x.test <- model.matrix(as.formula(final_formula), testData)[,-1]
predictions <- ridge %>% predict(x.test) %>% as.vector()
test_results$ridge=predictions

# Model performance metrics
# data.frame(
#  RMSE = RMSE(predictions, testData$new_deaths_smoothed_per_million),
#  Rsquare = R2(predictions, testData$new_deaths_smoothed_per_million)
# )
```

```{r}
# Elastic Net (alpha between 0 and 1)
# x = model.matrix(as.formula(final_formula), trainData)[,-1]
# y = trainData$new_deaths_smoothed_per_million
# #  Find the best lambda using cross-validation
# set.seed(123)
# cv <- cv.glmnet(x, y, alpha = 0.3)
# # Display the best lambda value
# #cv$lambda.min
# # Fit the final model on the training data
# elastic_net <- glmnet(x, y, alpha = 0.3, lambda = cv$lambda.min)
# save(elastic_net, file="C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/elastic_net.RData")

load("C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/elastic_net.RData")
# Display regression coefficients
#(coef(elastic_net)/coef(full_model)>0.5)
# Make predictions on the test data
x.test <- model.matrix(as.formula(final_formula), testData)[,-1]
predictions <- elastic_net %>% predict(x.test) %>% as.vector()
test_results$elastic_net=predictions
# Model performance metrics
# data.frame(
#  RMSE = RMSE(predictions, testData$new_deaths_smoothed_per_million),
#   Rsquare = R2(predictions, testData$new_deaths_smoothed_per_million)
# )
```

### Dimension Reduction Techniques

As said, PCR and PLS try to find the best subset of components. The following plot shows the number RMSE evolution with number of components. The more components, the more it improves. However, clearly PLS performs way better than PCR since only a few components of it work better than any PCR model.

```{r}
# Now with Caret:
# c7=makePSOCKcluster(7)
# registerDoParallel(c7)
# pcr_tune <- train(as.formula(final_formula), data = trainData,
#                   method='pcr',
#                   preProc=c('scale','center'),
#                   tuneGrid = expand.grid(ncomp = 2:30),#6 is the optimal, after, not much improvement
#                   trControl=ctrl)
# stopCluster(c7)
# save(pcr_tune, file="C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/pcr.RData")




# Partial Least Squares

# With Caret:
# 
# c7=makePSOCKcluster(7)
# registerDoParallel(c7)
# pls_tune <- train(as.formula(final_formula), data = trainData,
#                   method='pls',
#                   preProc=c('scale','center'),
#                   tuneGrid = expand.grid(ncomp = 2:30),
#                   trControl=ctrl)
# stopCluster(c7)
# save(pls_tune, file="C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/pls.RData")


```

```{r}
load("C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/pcr.RData")

test_results$pcr <- predict(pcr_tune, testData)
#postResample(pred = test_results$pcr,  obs = test_results$new_deaths_smoothed_per_million)


load("C:/Users/arpri/OneDrive/Escritorio/libros/master/4- Cuarto Semicuatrimestre/Regresión Avanzada y Predicción/pls.RData")
test_results$pls <- predict(pls_tune, testData)
#postResample(pred = test_results$pls,  obs = test_results$new_deaths_smoothed_per_million)
```


<p align="center">
```{r, fig.height=3, fig.width=3}
par(mfrow=c(1,2))
plot(pcr_tune, main="PCR")
plot(pls_tune, main="PLS")
```
</p>

### Final results

The final results from all the models are summarized in the following table: Root Mean Squared Error on the testing set as well as $R^2$ as a way to compare the variability explained by each of the models. Finally, prediction intervals are computed in a non-parametric way such that they are robust against outliers in order to check which percentage of the test sample lies inside (thus being covered inside the model prediction) or outside these intervals. This interval is calculated as

$$
\overline{y} \pm 2.4*\operatorname{mad}(\operatorname{noise})
$$
where $\overline{y}$ is the prediction of the model and mad(noise) is the mean absolute deviation of the noise, a measure of the error (standard deviation) in part of the testing set.

| Model | RMSE (test) | $R^2$ (test)| %test outside interval |
|:-----:|:----:|:--:|:------------------:|
|Simple L.R.|1.94 |0.62 | 82.8% |
| Full L. R. (no interactions)|1.87 |0.65 | 80.6% |
| Full L. R. (with interactions)|1.50 |0.77 |21.3% |
| Forward Regression| 1.75 | 0.69 | 38.8% |
| Backwards Regression|1.74 |0.70 | 42.7% |
| Stepwise Regression | 1.79 | 0.68 | 22.6% |
| Lasso |1.67 |0.72 | 21.4% |
| Ridge Regression |1.52 |0.77 | 24.0%|
| Elastic Net |1.53 |0.77 | 20.7%  |
| PCR |1.81 |0.67 | 20.5%|
| PLS |1.62 |0.74 | 22.9% |
| Ensemble (Elastic Net + PLS) |1.56 |0.75 |22.3% | 

We can recall that only with the new cases we were able to obtain around an $R^2=0.58$ and the full model with no interactions doesn't go much further beyond, only $R^2=0.65$. Clearly, the key to solving this problem is on interaction effects and non-linearities. Adding interaction effects improves greatly the model, as it can be seen in the table. Several algorithms have been used to filter the most important variables. Some of them, like Partial Least Squares, are able to maintain a similar performance to the full model with more than 200 variables with way less features. Also, an ensemble has been built with two of the best models: Elastic Net and PLS in order to generalize better to future unknown data.

On the other hand, in order to get explainability, instead of looking at the more than 200 variables that the full linear model with interactions chose, we can look at the ones that the stepwise model considered the most important, since it only has 16 variables, which is actually less than the total number of predictors (26, including dummies) and it still has an $R^2=0.68$ while having the same performance on the percentage of the test sample out of the prediction interval, around a 20%. Its coefficients are the following:

| $\beta_0$| $\beta_{Cases}$ |$\beta_{island}$  | $\beta_{Asia:Cases}$| $\beta_{Oceania:Month2}$ | $\beta_{Oceania:Month12}$ | $\beta_{cases:65older}$| $\beta_{cases:diabetes}$ | $\beta_{cases:l.exp}$|
|:-------------:|:-------------:|:------------:|:-----------:|:-----------:|:-------------:|:-----------:|:-----------:|:-----------:|
|1.6|13.6|-0.2|-0.5|0.004|0.008|0.3|0.08|-11.9|


| $\beta_{cases:Month2}$| $\beta_{cases:Month3}$ |$\beta_{cases:Month7}$ | $\beta_{cases:Month12}$ |$\beta_{Str.In:65older}$| $\beta_{Str.In:diabetes}$ | $\beta_{65older:Month3}$ | $\beta_{diabetes:Month3}$ |
:-------------:|:------------:|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|
|0.15|-0.02|0.06|0.1|0.6|0.1|-0.1|0.05|

This means that the explanation (excluding sophisticated non linear terms) of the problem is mainly based on the new cases and its interaction with the other variables. In particular, per unit increment it increases 13.6 on deaths but several corrections are then applied. The positive corrections are the interactions of cases with the percentage of old people, diabetes prevalence, February, July and December. The deaths are diminished, on the other hand, with cases interaction with Asia (each case in Asia is prone to 0.5 less deaths than in other parts of the world); with life expectancy it suffers a severe negative correction of -11.9 deaths (for a fixed number of cases) for unit of life expectancy. This means that each year of difference between life expectancy matters a lot not because the population is older (that in fact increases the chances of death from the interaction term between cases and % of 65 or older) but because it could be associated with richer countries with better healthcare systems, for example. Finally, each case in March also causes 0.02 less deaths than in other months.

On the other hand, clearly the incorporation of the new feature "is island" was very successful, being in this selection of the most important variables. It is shown that islands have in general 0.2 deaths per million less, which could be attributed to their ease in isolating themselves and reducing transmission with the rest of the world.

Finally, it can be seen that other interactions like Oceania in February or December, stringency index with old people and diabetes as well as the fact that the effect of diabetes in March is amplified and on old people, reduced.

```{r, eval=FALSE}

# Combination
test_results$comb = (test_results$pls+ test_results$elastic_net)/2
#postResample(pred = test_results$comb,  obs = test_results$new_deaths_smoothed_per_million)



for(names in colnames(test_results)){
   if(names!="new_deaths_smoothed_per_million"){
      # Final predictions:
      yhat = lapply(test_results[names],as.numeric)[[names]]
      #head(yhat) # show the prediction for 6 home prices
      #hist(yhat, col="lightblue")
      # take care: asymmetric distribution
      
      y = test_results$new_deaths_smoothed_per_million
      error = y-yhat
      #hist(error, col="lightblue")
      # But the error is more symmetric
      
      # But we cannot use the information in the testing set to obtain the confidence intervals in the testing set
      # Hence: split the testing set in two parts, one to measure the size of the noise, and the other one to compute the CIs from that size
      
      # Let's use the first 100 homes in testing to compute the noise size
      noise = error[1:100]
      sd.noise = sd(noise)
      
      # Non-parametric way, even more robust against outliers
      lwr = yhat[101:length(yhat)] - 2.4*mad(noise) 
      upr = yhat[101:length(yhat)] + 2.4*mad(noise)
      # for normal data, sd=1.48*mad
      
      predictions = data.frame(real=y[101:length(y)], fit=yhat[101:length(yhat)], lwr=lwr, upr=upr)
      predictions = predictions %>% mutate(out=factor(if_else(real<lwr | real>upr,1,0)))
      
      # how many real observations are out of the intervals?
      print(paste(c("percentage of real observations out of the interval for", names)))
      print(mean(predictions$out==1))
      # prediction of spanish value 0.35--> interval?
      print(0.35-2.4*mad(noise))
      print(0.35+2.4*mad(noise))

   }
}



```

## Discussion and Conclusions


In conclusion, the methodology used has been able to obtain models that achieve an explanation of 77% of the variance of the COVID-19 7-day average deaths per million as well as obtaining the main factors that explain it: the number of cases and the fact that the country is an island, together with corrections of the number of old people, diabetes prevalence, restrictive measures and the month of the year.

On the other hand, there is still some part of the problem that are unexplained by the model since it was difficult to obtain exact variable transformations that could represent more complex non linear effects. An analysis with Machine Learning tools would be needed in order to find these underlying patterns.

Finally, we can use this knowledge not only to explain the main features that drove the pandemic's death but also to make predictions. The data used stop at mid April 2021, so it could be used, for example, to predict the situation this summer (July). Since most of the models have similar performance, I will use the full model because it includes also vaccine information. See the results plugging in Spain's characteristics, which are 50% of vaccinated people by July (following government estimation); 1000 cases on average (20 per million inhabitants of Spain) knowing that there are now around 10000 and last July it descended to around 1000 cases; Stringency Index with low measures, 30. HDI of 0.904 ; population density 93.105; 19.436% of people older than 65; cardiovascular death rate of 99.403; diabetes prevalence of 7.17; life expectancy of 83.56 years and on month 7 (July), in Europe and not an island. Furthermore, the same confidence interval that were estimated for the out of sample testing set can be used (using the same noise value obtained for the full model) yielding a low number of deaths of [-0.7, 1.4], compatible with 0 deaths (we shall see this prediction in summer).

```{r, eval=FALSE}
spain=data.frame(NA)
spain$people_fully_vaccinated_per_hundred=50
spain$new_cases_smoothed_per_million=20.0
spain$stringency_index=30.0
spain$human_development_index=0.904
spain$population_density=93.105
spain$aged_65_older=19.436
spain$cardiovasc_death_rate=99.403
spain$diabetes_prevalence=7.17
spain$life_expectancy=83.56
spain$continent="Europe"
spain$month="7"
spain$is_island="no"
spain=spain[,-1]

predict(full_model, spain)
```